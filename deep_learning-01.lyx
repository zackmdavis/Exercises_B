#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Exercises in Deep Learning
\end_layout

\begin_layout Author
Zack M.
 Davis
\end_layout

\begin_layout Date
December 2025
\end_layout

\begin_layout Abstract
Exercises from 
\emph on
Deep Learning: Foundations and Concepts
\emph default
 by Christopher Bishop with Hugh M.
 Bishop.
\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\var}{\mathrm{var}}
\end_inset


\end_layout

\begin_layout Standard

\series bold
2.1
\series default
.
 The classical medical test Bayes example: we have 
\begin_inset Formula $p(C=1)=0.001$
\end_inset

, 
\begin_inset Formula $p(T=1|C=1)=0.9$
\end_inset

, and 
\begin_inset Formula $p(T=1|C=0)=0.03$
\end_inset

.
 Bayes says:
\begin_inset Formula 
\[
p(C=1|T=1)=\frac{p(T=1|C=1)P(C=1)}{p(T=1|C=1)P(C=1)+p(T=1|C=0)P(C=0)}=\frac{0.9\cdot0.001}{0.9\cdot0.001+0.03\cdot0.999}\approx0.029
\]

\end_inset


\end_layout

\begin_layout Standard
(Looking at the solution manual reminded me that I hadn't written the correct
 
\begin_inset Formula $p(T=1|C=0)$
\end_inset

.)
\end_layout

\begin_layout Standard

\series bold
2.2
\series default
.
 Non-transitive dice.
 
\emph on
All-3s vs.
 4,4,4,4,0,0
\emph default
.
 
\begin_inset Formula $\frac{2}{3}\cdot4>3$
\end_inset

, but 
\begin_inset Formula $\frac{1}{3}\cdot0<3$
\end_inset

.
\end_layout

\begin_layout Standard

\emph on
4,4,4,4,0,0 vs.
 5,5,5,1,1,1.

\emph default
 
\begin_inset Formula $\frac{1}{2}\cdot5$
\end_inset

 is greater than anything on the 0/4 die.
 The other half of the time, we get a 
\begin_inset Formula $1$
\end_inset

 on the 1/5 die, and that beats the 0/4 die 
\begin_inset Formula $\frac{1}{3}$
\end_inset

 of the time.
 So the total probability of the 1/5 die winning is 
\begin_inset Formula $\frac{1}{2}+\frac{1}{2}\cdot\frac{1}{3}=\frac{3}{6}+\frac{1}{6}=\frac{4}{6}=\frac{2}{3}$
\end_inset

.
\end_layout

\begin_layout Standard

\emph on
5,5,5,1,1,1 vs.
 2,2,2,2,6,6
\emph default
.
 
\begin_inset Formula $6$
\end_inset

 is greater than everything on the 1/5 die, so the 2/6 die wins 
\begin_inset Formula $\frac{1}{3}$
\end_inset

 of the time on that account.
 The other 
\begin_inset Formula $\frac{2}{3}$
\end_inset

 of the time, it wins half the time (depending on whether the 1/5 die came
 up 
\begin_inset Formula $1(<2)$
\end_inset

 or 
\begin_inset Formula $5(>2)$
\end_inset

.
\end_layout

\begin_layout Standard

\emph on
2,2,2,2,6,6 vs.
 3,3,3,3,3,3
\emph default
.
 The 2/6 die comes up 
\begin_inset Formula $2$
\end_inset

 
\begin_inset Formula $\frac{2}{3}$
\end_inset

 of the time (and loses), otherwise 
\begin_inset Formula $6$
\end_inset

 (and it wins).
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\series bold
2.3
\series default
.
 We want to show that the distribution for the sum of random variables 
\begin_inset Formula $y=u+v$
\end_inset

 is given by a convolution.
 For any known 
\begin_inset Formula $u$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

, we know that 
\begin_inset Formula $v$
\end_inset

 must take the value 
\begin_inset Formula $y-u$
\end_inset

.
 Consider a given value of 
\begin_inset Formula $u$
\end_inset

, with probability 
\begin_inset Formula $p_{u}(u)$
\end_inset

.
 The contribution to the probability of a given value of 
\begin_inset Formula $y$
\end_inset

 that is supplied by this value of 
\begin_inset Formula $u$
\end_inset

 is then 
\begin_inset Formula $p_{u}(u)$
\end_inset

 multiplied by the probability of the corresponding 
\begin_inset Formula $v$
\end_inset

 given by 
\begin_inset Formula $v:=y-u$
\end_inset

.
 To get the total probability of 
\begin_inset Formula $y$
\end_inset

, we sum/integrate over all such values of 
\begin_inset Formula $u$
\end_inset

: 
\begin_inset Formula $p(y)=\int p_{u}(u)p_{v}(y-u)\,du$
\end_inset

.
\end_layout

\begin_layout Standard
I think that's fine as an informal explanation, but the solution manual's
 version is probably better: the law of conditional probability yields 
\begin_inset Formula $p(y)=\iint p(y|u,v)p_{u}(u)p_{v}(v)\,du\,dv$
\end_inset

, and then they swap out the conditional factor with a Dirac delta 
\begin_inset Formula $p(y|u,v)=\delta(y-u-v)$
\end_inset

.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\series bold
2.4
\series default
.
 We verify that the uniform distribution is normalized: 
\begin_inset Formula $\int_{c}^{d}\frac{1}{d-c}\,dx=\left.\frac{1}{d-c}x\right|_{x=c}^{d}=\frac{d}{d-c}-\frac{c}{d-c}=\frac{d-c}{d-c}=1$
\end_inset

.
\end_layout

\begin_layout Standard
We verify its mean: 
\begin_inset Formula $E[x]=\int_{c}^{d}\frac{1}{d-c}x\:dx=\left.\frac{1}{2}\frac{1}{d-c}x^{2}\right|_{x=c}^{d}=\frac{1}{2}\frac{1}{d-c}(d^{2}-c^{2})=\frac{\cancel{(d-c)}(d+c)}{2\cancel{(d-c)}}=\frac{d+c}{2}$
\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Commentary
\emph default
.
 For such a familiar and intuitive result, the derivation feels surprising;
 I didn't expect it to go through a quadratic and the difference of squares.
\end_layout

\begin_layout Standard
We compute the variance as 
\begin_inset Formula $E[x^{2}]-E[x]^{2}$
\end_inset

.
 
\end_layout

\begin_layout Standard
We compute 
\begin_inset Formula $E[x^{2}]=\int_{c}^{d}\frac{1}{d-c}x^{2}=\frac{1}{3}\frac{1}{d-c}\left.x^{3}\right|_{x=c}^{d}=\frac{1}{3}\frac{1}{d-c}\left(d^{3}-c^{3}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
We compute 
\begin_inset Formula $E[x]^{2}=\left(\frac{d+c}{2}\right)^{2}=\frac{d^{2}+2cd+c^{2}}{4}$
\end_inset


\end_layout

\begin_layout Standard
Then 
\begin_inset Formula $E[x^{2}]-E[x]^{2}=\frac{1}{3}\frac{1}{d-c}\left(d^{3}-c^{3}\right)-\frac{1}{4}\left(d^{2}+2cd+c^{2}\right)$
\end_inset

.
 (Here I had some doubt that we had strayed from the true path, but looking
 at the solution manual taught me to have more faith; we can keep simplifying
 from here.) 
\end_layout

\begin_layout Standard
\begin_inset Formula $\frac{\left(d^{3}-c^{3}\right)}{3(d-c)}-\frac{d^{2}+2cd+c^{2}}{4}=\frac{4\left(d^{3}-c^{3}\right)}{12(d-c)}-\frac{3\left(d^{2}+2cd+c^{2}\right)(d-c)}{12(d-c)}=\frac{4\left(d^{3}-c^{3}\right)}{12(d-c)}-\frac{3\left(d^{3}+2cd^{2}+dc^{2}-cd^{2}-2c^{2}d-c^{3}\right)}{12(d-c)}=\frac{\cancel{4}d^{3}-\cancel{4}c^{3}\cancel{-3d^{3}}-6cd^{2}-3dc^{2}+3cd^{2}+6c^{2}d\cancel{+3c^{3}}}{12(d-c)}=\frac{d^{3}-c^{3}-6cd^{2}-3dc^{2}+3cd^{2}+6c^{2}d}{12(d-c)}$
\end_inset

 ...
 and maybe you need a polynomial long division there to finish shaking it
 out? [
\series bold
TODO
\series default
]
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\series bold
2.5
\series default
.
 We verify that the exponential and Laplace distributions are normalized.
\end_layout

\begin_layout Standard
For the exponential, we have 
\begin_inset Formula $\int_{0}^{\infty}\lambda\exp(-\lambda x)\,dx=\cancel{\lambda}\cdot\frac{1}{-\cancel{\lambda}}\left.\exp(-\lambda x)\right|_{x=0}^{\infty}=\lim_{t\to\infty}-1\cdot\left(\exp(-\lambda t)-\exp(0)\right)=\lim_{t\to\infty}1-\exp(-\lambda t)=1$
\end_inset

.
 
\begin_inset Formula $\checkmark$
\end_inset


\end_layout

\begin_layout Standard
For the Laplace, we have 
\begin_inset Formula $\int_{-\infty}^{\infty}\frac{1}{2\gamma}\exp(-\frac{|x-\mu|}{\gamma})$
\end_inset

 which we can 
\begin_inset Quotes eld
\end_inset

recenter
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

demirror
\begin_inset Quotes erd
\end_inset

 to 
\begin_inset Formula $\cancel{2\frac{1}{2}}\int_{0}^{\infty}\frac{1}{\gamma}\exp(-\frac{1}{\gamma}x)$
\end_inset

, and then goes just the same as the exponential (with the parameter written
 as 
\begin_inset Formula $\frac{1}{\gamma}$
\end_inset

 instead of 
\begin_inset Formula $\lambda$
\end_inset

).
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\series bold
2.9
\series default
.
 
\emph on
Proposition
\emph default
.
 Independent random variables have a covariance of zero.
\end_layout

\begin_layout Standard

\emph on
Proof
\emph default
.
 
\begin_inset Formula $\mathrm{cov}[x,y]=E_{x,y}[(x-E[x])(y-E[y])]=E_{x,y}[xy-xE[y]-yE[x]+E[x]E[y]]$
\end_inset

, and then by the linearity of expectation, we have 
\begin_inset Formula $E_{x,y}[xy]-E_{x,y}[xE[y]]-E_{x,y}[yE[x]]+E[x]E[y]]$
\end_inset

, which is 
\begin_inset Formula $E_{x,y}[xy]-2E[x]E[y]+E[x]E[y]=E_{x,y}[xy]-E[x]E[y]$
\end_inset

.
 If 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 are independent, 
\begin_inset Formula $E_{x,y}[xy]=\sum_{x}\sum_{y}xy\cdot p(x,y)=\sum_{x}\sum_{y}xy\cdot p(x)\cdot p(y)=\sum_{x}x\cdot p(x)\cdot\sum_{y}y\cdot p(y)=E[x]E[y]$
\end_inset

.
 
\begin_inset Formula $\checkmark$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\series bold
2.10
\series default
.
 
\emph on
Theorem
\emph default
.
 Expectation is linear.
\end_layout

\begin_layout Standard

\emph on
Proof
\emph default
.
 
\begin_inset Formula $E_{X,Y}[X+Y]=\sum_{x\in X,y\in Y}p(x,y)(x+y)=\sum_{x\in X}\sum_{y\in Y}x\cdot p(x,y)+\sum_{x\in X}\sum_{y\in Y}y\cdot p(x,y)=\sum_{x\in X}x\cdot p(x)+\sum_{y\in Y}y\cdot p(x,y)=E[X]+E[Y]$
\end_inset

 
\begin_inset Formula $\checkmark$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\emph on
Theorem
\emph default
.
 Variance of independent random variables is linear.
\end_layout

\begin_layout Standard

\emph on
Proof
\emph default
.
 
\begin_inset Formula $\mathrm{var}(X+Y)=E[(X+Y)^{2}]-E[X+Y]^{2}=E[(X+Y)^{2}]-\left(E[X]+E[Y]\right)^{2}=E[(X+Y)^{2}]-\left(E[X]^{2}+2E[X]E[Y]+E[Y]^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
But 
\begin_inset Formula $E[(X+Y)^{2}]=E[X^{2}+2XY+Y^{2}]=E[X^{2}]+2E[XY]+E[Y]^{2}$
\end_inset

 by the linearity of expectation.
\end_layout

\begin_layout Standard
Then 
\begin_inset Formula $E[XY]=\sum_{x\in X,y\in Y}xy\cdot p(x,y)$
\end_inset

 but independence means that 
\begin_inset Formula $p(x,y)=p(x)p(y)$
\end_inset

, so we have 
\begin_inset Formula $\sum_{x\in X}\sum_{y\in Y}xy\cdot p(x)\cdot p(y)=\sum_{x\in X}x\cdot p(x)\sum_{y\in Y}y\cdot p(y)=E[X]E[Y]$
\end_inset

.
\end_layout

\begin_layout Standard
Thus, we see that ultimately, the cross-terms cancel: 
\begin_inset Formula $E[X^{2}]\cancel{+2E[X]E[Y]}+E[Y]^{2}-E[X]^{2}\cancel{-2E[X]E[Y]}-E[Y]^{2}=\underbrace{E[X^{2}]-E[X]^{2}}_{\mathrm{var}(X)}+\underbrace{E[Y]^{2}-E[Y]^{2}}_{\mathrm{var}(Y)}$
\end_inset

 
\begin_inset Formula $\checkmark$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\series bold
2.11
\series default
.
 
\emph on
Theorem
\emph default
.
 
\begin_inset Formula $E[X]=E_{Y}[E_{X}[X|Y]]$
\end_inset


\end_layout

\begin_layout Standard

\emph on
Proof
\emph default
.
 
\begin_inset Formula $E_{y}[E_{x}[X|Y]]=\sum_{y}E[X|Y=y]\cdot p(y)=\sum_{y}\sum_{x}x\cdot p(x|y)\cdot p(y)=\sum_{x}x\sum_{y}p(x|y)p(y)=\sum_{x}x\cdot p(x)=E[X]$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\emph on
Theorem
\emph default
.
 
\begin_inset Formula $\var(X)=E_{Y}[\var_{X}[X|Y]]+\var_{Y}[E_{X}[X|Y]]$
\end_inset


\end_layout

\begin_layout Standard

\emph on
Commentary
\emph default
.
 This decomposition ought to have some philosophy behind it, which is currently
 opaque to me.
\end_layout

\begin_layout Standard

\emph on
Proof
\emph default
.
 
\begin_inset Formula $E_{Y}[\var_{X}[X|Y]]=\sum_{y\in Y}\var_{X}[X|Y=y]\cdot p(y)=\sum_{y\in Y}\left(E[X^{2}|Y=y]-E[X|Y=y]^{2}\right)\cdot p(y)$
\end_inset

 [
\series bold
TODO
\series default
]
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\series bold
2.15
\series default
.
 The likelihood of an i.i.d.
 Gaussian dataset is 
\begin_inset Formula $p(\boldsymbol{x}|\mu,\,\sigma^{2})=\prod_{j=1}^{N}\mathcal{N}(x_{j}|\mu,\,\sigma^{2})=\prod_{j=1}^{N}\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{1}{2\sigma^{2}}(x_{j}-\mu)^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The log likelihood is 
\begin_inset Formula $\sum_{j=1}^{N}\log\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{1}{2\sigma^{2}}(x_{j}-\mu)^{2}\right)=\sum_{n=1}^{N}\log\frac{1}{\sqrt{2\pi\sigma^{2}}}+\cancel{\log\exp}\left(-\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right)=\sum_{n=1}^{N}$
\end_inset

 [
\series bold
TODO
\series default
]
\end_layout

\end_body
\end_document
