#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Notes on Singular Learning Theory
\end_layout

\begin_layout Date
December 2025
\end_layout

\begin_layout Subsection*
Introduction
\end_layout

\begin_layout Standard
A statistical model is some procedure that we use to make predictions about
 data.
 We usually start off by considering some parametrized class of model and
 then think about how the parameters should be set in order to make the
 best predictions.
 A simple example that you're already familiar with is linear regression:
 if we have a bunch of data points 
\begin_inset Formula $(x_{j},y_{j})$
\end_inset

 from the past, and we want to figure out how to predict future 
\begin_inset Formula $y_{j}$
\end_inset

 given only the corresponding 
\begin_inset Formula $x_{j}$
\end_inset

, we can use 
\begin_inset Formula $y=mx+b$
\end_inset

 as our model class with two parameters 
\begin_inset Formula $m$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

.
 The 
\begin_inset Quotes eld
\end_inset

best
\begin_inset Quotes erd
\end_inset

 parameters are the ones that minimize the sum of the squares of the deviations
 from the true value: 
\begin_inset Formula $\sum(y_{\mathrm{predicted}}-y_{\mathrm{actual}})^{2}=\sum_{k}(mx_{k}+b-y_{k})^{2}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Naturally, if our data is being generated by something more complicated
 than a line, we want a more expressive model class than linear regression
 in order to fit the data.
\end_layout

\begin_layout Standard
There's a trade-off: more complex models can fit more complicated patterns
 in the data, but the more complicated the model is, the more data you need
 in order to pin down which parameters are good.
 If your model class is degree-15 polynomials, you can get a perfect fit
 to any 16 data points, but that's not going to be a good model unless you're
 perfectly confident the data actually came from that exact degree-15 polynomial
: the perfect fit is probably 
\emph on
overfitting
\emph default
.
 The famous 
\emph on
bias–variance trade-off
\emph default
 is an expression of how if there's any randomness in the data-generating
 process at all, you're going to get very different results if you fit the
 model again (
\begin_inset Quotes eld
\end_inset

variance
\begin_inset Quotes erd
\end_inset

), which can be compensated for by using a less expressive model class with
 stronger preconceptions of what the data could be like (
\begin_inset Quotes eld
\end_inset

bias
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_layout Standard
The 
\emph on
minimum description length principle
\emph default
 is another manifestation of the trade-off between simplicity and fit.
 In MDL, the best models are the ones that minimize the length of the message
 you would need to describe the conjunction of your model and the data.
 More complicated models take more information-theoretic bits to describe,
 but they can 
\begin_inset Quotes eld
\end_inset

pay
\begin_inset Quotes erd
\end_inset

 for the extra bit-cost of their complexity by making better predictions
 that can be used to compress the data.
\end_layout

\begin_layout Standard
Traditionally, people thought the 
\begin_inset Quotes eld
\end_inset

complexity
\begin_inset Quotes erd
\end_inset

 of a model class corresponded to its number of parameters: more parameters,
 more complexity.
 But the deep learning revolution of the 2010s calls this into question,
 because neural networks generalize much better than they 
\begin_inset Quotes eld
\end_inset

should
\begin_inset Quotes erd
\end_inset

 according to classical theory.
 
\end_layout

\begin_layout Standard
A striking example is the puzzle of 
\emph on
double descent
\emph default
: the classical theory would expect models to overfit (like a high-degree
 polynomial) as the number of parameters approaches the number of training
 examples, but it turns out that if you keep increasing the number of parameters
 past the point where the model can memorize the training data, you actually
 get better performance, which should be impossible.
\end_layout

\begin_layout Standard
Another potential example is the existence of LoRAs: you can fine-tune a
 network's high-level behavior while only optimizing a tiny number of additional
 parameters.
\end_layout

\begin_layout Standard
But how can this be possible, if the complexity of a statistical model is
 measured by its number of parameters? Somehow, the fact that deep learning
 generalizes so well must imply that the 
\begin_inset Quotes eld
\end_inset

true
\begin_inset Quotes erd
\end_inset

 complexity measure is something other than just counting the number of
 parameters.
 That's possible because for deep neural networks, many different parametrizatio
ns have the same input–output function.
 (Note that this is not true for linear regression: every distinct choice
 of slope and intercept is a distinct line.) The network architecture is
 biased towards simple functions: a generic random setting of the parameters
 represents a simpler function than the network is capable of representing.
 (Note that this is not true for polynomials: if you choose the coëfficients
 of a degree-15 polynomial randomly, the resulting function is not going
 to be a quadratic except by an astronomical coincidence that the coëfficients
 on 
\begin_inset Formula $x^{3}$
\end_inset

 to 
\begin_inset Formula $x^{15}$
\end_inset

 just happened to be zero.) Rather than complexity being determined by the
 model architecture itself, different regions of parameter-space have different
 complexity, such that the network can dynamically choose 
\begin_inset Quotes eld
\end_inset

how complex
\begin_inset Quotes erd
\end_inset

 it should be as it's trained.
\end_layout

\begin_layout Standard
Singular learning theory tells us how to compute the 
\begin_inset Quotes eld
\end_inset

true
\begin_inset Quotes erd
\end_inset

 complexity.
 A lot of classical theory assumes 
\emph on
regular
\emph default
 statistical models where the parameters determine the input–output function,
 and therefore, that the optimal parameter setting is a single point: the
 loss landscape is a bowl with an isolated minimum.
 In 
\emph on
singular
\emph default
 models, minimum can be some more complicated curve or shape rather than
 a single point.
\end_layout

\begin_layout Standard
The 
\begin_inset Quotes eld
\end_inset

true complexity
\begin_inset Quotes erd
\end_inset

 is called the real log canonical threshold (RLCT), or local learning coëfficien
t.
 The learning coëfficient is like 
\begin_inset Quotes eld
\end_inset

flatness
\begin_inset Quotes erd
\end_inset

 in the loss landscape.
 In the limit of infinite data, the most accurate parametrization wins,
 but for finite amounts of data, regions with a better accuracy/simplicity
 trade-off can be favored.
\end_layout

\begin_layout Subsection*
The Bayesian Context
\end_layout

\begin_layout Standard
A deep network being trained by stochastic gradient descent occupies a particula
r point in parameter-space at any given time, but we can imagine the ideal
 Bayesian probability distribution over parameterizations: start with a
 prior distribution over parameter-space, and then update it on data.
 As data comes in, probability-mass shifts to favor the true parameters,
 but favoring regions with a low RLCT.
 Our neural network learns to be a pretty-good easy-to-find model, before
 it learns to be better, harder-to-find model.
\end_layout

\begin_layout Standard
Recall Bayes's theorem 
\begin_inset Formula $P(H_{i}|E)=\frac{P(E|H_{i})P(H_{i})}{P(E)}=\frac{P(E|H_{i})P(H_{i})}{\sum_{j}P(E|H_{j})P(H_{j})}$
\end_inset

.
 Let's adapt that form of Bayes to the setting of singular learning theory.
\end_layout

\begin_layout Standard
Instead of a collection of hypotheses 
\begin_inset Formula $\{H_{i}\}_{i}$
\end_inset

 to sum over, we'll consider settings of weight parameters 
\begin_inset Formula $w$
\end_inset

 to integrate over.
\end_layout

\begin_layout Standard
Our evidence will be a dataset of size 
\begin_inset Formula $n$
\end_inset

, 
\begin_inset Formula $D_{n}$
\end_inset

.
\end_layout

\begin_layout Standard
We have a prior distribution 
\begin_inset Formula $\varphi(w)$
\end_inset

 on the weights.
\end_layout

\begin_layout Standard
The likelihood 
\begin_inset Formula $P(E|H_{i})$
\end_inset

 becomes the probability of the dataset given the weights, 
\begin_inset Formula $P(D_{n}|w)$
\end_inset

.
\end_layout

\begin_layout Standard
So our posterior for the weights given the data is
\begin_inset Formula 
\[
p(w|D_{n})=\frac{p(D_{n}|w)\varphi(w)}{\int p(D_{n}|w)\varphi(w)\,dw}
\]

\end_inset


\end_layout

\begin_layout Standard
We're interested in computing the denominator 
\begin_inset Formula $Z_{n}:=\int p(D_{n}|w)\varphi(w)\,dw$
\end_inset

 for large 
\begin_inset Formula $n$
\end_inset

.
 This is a very high-dimensional integral that is hard to compute.
\end_layout

\begin_layout Standard
We're interested in 
\begin_inset Formula $Z_{n}$
\end_inset

 because, by virtue of being the average likelihood over weight-settings,
 it's saying something about how good our model architecture is fitting
 the observed data: higher 
\begin_inset Formula $Z_{n}$
\end_inset

 means there's more prior probability-mass on weight-settings that make
 the data seem likely.
\end_layout

\begin_layout Standard
Suppose our datapoints 
\begin_inset Formula $(x_{i},y_{i})$
\end_inset

 are independent.
 Then the likelihood 
\begin_inset Formula $p(D_{n}|w)$
\end_inset

 is a product 
\begin_inset Formula $\prod_{i=1}^{n}p(y_{i}|x_{i},w)$
\end_inset

.
 We'd prefer to work with a sum (and by convention to minimize error rather
 than maximize probability), so we're going to use the (average) negative
 log likelihood as our loss function: 
\begin_inset Formula $L_{n}(w)=-\frac{1}{n}\sum_{i=1}^{n}\log p(y_{i}|x_{i},w)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L_{n}(w)=-\frac{1}{n}\sum_{i=1}^{n}\log p(y_{i}|x_{i},w)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
-nL_{n}(w)=\sum_{i=1}^{n}\log p(y_{i}|x_{i},w)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\exp(-nL_{n}(w))=\exp\left(\sum_{i=1}^{n}\log p(y_{i}|x_{i},w)\right)=\prod_{i=1}^{n}\cancel{\exp}\left(\cancel{\log}p(y_{i}|x_{i},w)\right)=\prod_{i=1}^{n}p(y_{i}|x_{i},w)=p(D_{n}|w)
\]

\end_inset


\end_layout

\begin_layout Standard
Thus, we have shown that given the negative log likelihood loss function,
 the likelihood can be written as 
\begin_inset Formula $\exp(-nL_{n}(w))$
\end_inset

.
\end_layout

\begin_layout Standard
So, we want to evaluate the integral 
\begin_inset Formula 
\[
Z_{n}=\int\exp(-nL_{n}(w))\varphi(w)\,dw
\]

\end_inset

.
 
\end_layout

\begin_layout Standard
For regular models and for large 
\begin_inset Formula $n$
\end_inset

, only a neighborhood around the minimum 
\begin_inset Quotes eld
\end_inset

matters
\begin_inset Quotes erd
\end_inset

 for the integral, because everything else is getting crushed to zero exponentia
lly.
 
\end_layout

\begin_layout Standard
Because only the local neighborhood matters, we can use the Taylor expansion.
 At the minimum 
\begin_inset Formula $w_{0}$
\end_inset

, the first derivative of the loss function 
\begin_inset Formula $L(w)$
\end_inset

 is zero; the first nonzero term is the quadratic.
 Because we're looking at a tiny 
\begin_inset Quotes eld
\end_inset

spiky
\begin_inset Quotes erd
\end_inset

 neighborhood, we can pretend the prior is constant and make-believe that
 our integral is 
\begin_inset Formula $\int\exp(-nL_{n}(w))\,dw$
\end_inset

.
 The quadratic approximation is 
\begin_inset Formula $\int\exp(-n\cdot\frac{1}{2}w^{T}Hw)$
\end_inset

, where 
\begin_inset Formula $H$
\end_inset

 is the second derivative matrix of 
\begin_inset Formula $L(w)$
\end_inset

.
 The value of this 
\begin_inset Quotes eld
\end_inset

Gaussian integral
\begin_inset Quotes erd
\end_inset

 is known to be proportional to 
\begin_inset Formula $\frac{1}{\sqrt{\det H}}$
\end_inset

 (derivation deferred).
\end_layout

\begin_layout Standard
For singular models, this approximation doesn't work, because 
\begin_inset Formula $\det H=0$
\end_inset

: the higher-than-quadratic terms matter.
 Instead of the minimum of the loss function being a single point, it's
 going to be an algebraic variety.
 A variety can have singularities—cusps or self-intersections—and it turns
 out that these dominate the integral.
 To calculate the integral, we need Hironaka's technique on the resolution
 of singularities.
\end_layout

\begin_layout Subsection*
First Example
\end_layout

\begin_layout Standard
Let's consider the simplest possible case: a neural network with one hidden
 layer of one neuron, activating linearly: 
\begin_inset Formula $f(x)=w_{2}w_{1}x$
\end_inset

.
 (There's a super annoying convention where the diagrams often depict 
\begin_inset Quotes eld
\end_inset

neurons
\begin_inset Quotes erd
\end_inset

 connected by lines, but the actual weights are the lines in the diagram.
 Input, then hidden layer, then output, is two weights, not three.) Suppose
 the true function is 
\begin_inset Formula $f(x)=0$
\end_inset

, and we're using squared-error loss 
\begin_inset Formula $L(w_{1},w_{2},x)=(w_{1}w_{2}x-0)^{2}=w_{1}^{2}w_{2}^{2}x^{2}$
\end_inset

.
 We can marginalize over 
\begin_inset Formula $x$
\end_inset

 to get a constant that doesn't matter and just work with 
\begin_inset Formula $L(w)=w_{1}^{2}w_{2}^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
The solution to 
\begin_inset Formula $w_{1}w_{2}=0$
\end_inset

 is the union of the 
\begin_inset Formula $w_{1}$
\end_inset

 and 
\begin_inset Formula $w_{2}$
\end_inset

 axes.
 The Hessian is 
\begin_inset Formula $\left[\begin{array}{cc}
\frac{\partial^{2}L}{\partial w_{1}^{2}} & \frac{\partial^{2}L}{\partial w_{1}\partial w_{2}}\\
\frac{\partial^{2}L}{\partial w_{2}\partial w_{1}} & \frac{\partial^{2}L}{\partial w_{2}^{2}}
\end{array}\right]=\left[\begin{array}{cc}
2w_{2}^{2} & 4w_{1}w_{2}\\
4w_{1}w_{2} & 2w_{1}^{2}
\end{array}\right]$
\end_inset

, which is 0 at the origin.
\end_layout

\begin_layout Standard
We make a new coordinate chart that maps to our weightspace: 
\begin_inset Formula $\begin{cases}
w_{1}=u\\
w_{2}=uv
\end{cases}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Then 
\begin_inset Formula $L$
\end_inset

 in terms of 
\begin_inset Formula $u$
\end_inset

 and 
\begin_inset Formula $v$
\end_inset

 becomes 
\begin_inset Formula $u^{2}(uv)^{2}=u^{4}v^{2}$
\end_inset

.
 
\end_layout

\begin_layout Standard
And we also need to include the Jacobian for the change-of-variables: 
\begin_inset Formula $\left|\begin{array}{cc}
\frac{\partial w_{1}}{\partial u} & \frac{\partial w_{1}}{\partial v}\\
\frac{\partial w_{2}}{\partial u} & \frac{\partial w_{2}}{\partial v}
\end{array}\right|=\left|\begin{array}{cc}
1 & 0\\
v & u
\end{array}\right|=u$
\end_inset

.
\end_layout

\begin_layout Standard
So our integral becomes 
\begin_inset Formula $Z_{n}=\int\exp(-nL(u,v))\cdot|J(u,v)|\,du\,dv=\int\exp(-nu^{4}v^{2})\cdot u\,du\:dv$
\end_inset

.
\end_layout

\end_body
\end_document
